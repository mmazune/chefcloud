# M12.4 Hypotheses

**Milestone**: M12.4  
**Date**: 2026-01-07  
**Status**: Pre-Implementation  
**Prior Commit**: `2208e7e` (M12.3)

---

## H1: Close Requests Allow Cross-Branch Leakage

**Hypothesis**: If org/branch filters are missing from close request queries, users could see or approve close requests from other organizations/branches.

**Confidence**: HIGH (95%)

**Failure Mode**: Cross-tenant data leakage, compliance violation.

**Validation**:
- Every query must include `orgId` from auth context
- E2E test: Create request in org A, verify org B user cannot list/approve it
- Use existing `factory.orgId` and `factory.branchId` patterns

**Mitigation**: All repository methods accept `orgId` as required parameter; controller extracts from JWT.

---

## H2: L5 Force-Close Bypass Lacks Event Logging

**Hypothesis**: If L5 uses forceClose to bypass approval workflow, the action may not be properly logged as an audit event, creating a compliance gap.

**Confidence**: HIGH (90%)

**Failure Mode**: No audit trail for overrides, SOX compliance risk.

**Validation**:
- Force-close must write `FORCE_CLOSE_USED` event to InventoryPeriodEvent
- Event must include reason (≥10 chars), actorUserId, metadataJson
- E2E test: Force-close period, verify event exists with correct data

**Mitigation**: Service method validates reason length, creates event before executing close.

---

## H3: Dashboard Preclose Aggregation Is Slow / N+1 Query

**Hypothesis**: The dashboard endpoint that aggregates preclose status across multiple branches may trigger N+1 queries (one per branch), causing performance issues.

**Confidence**: MEDIUM (70%)

**Failure Mode**: Dashboard timeout, poor UX for orgs with many branches.

**Validation**:
- Monitor query count in E2E test with multiple branches
- Use batch queries: fetch all periods, all requests, all last events in single queries
- Return aggregated results

**Mitigation**: Use Prisma `findMany` with proper `where` clauses, aggregate in memory. Consider adding `@@index([orgId, status])` on close requests.

---

## H4: Alert Idempotency Fails → Duplicates on Repeated Run-Preclose

**Hypothesis**: If `runPreclose` is called multiple times and creates PERIOD_CLOSE_BLOCKED alerts, duplicate alerts may be created if idempotency logic fails.

**Confidence**: HIGH (85%)

**Failure Mode**: Alert spam, user confusion, missed alerts due to noise.

**Validation**:
- Use existing InventoryAlert unique constraint: `[orgId, branchId, type, entityType, entityId, status]`
- Upsert pattern: find existing OPEN alert with same key, update if exists, create if not
- E2E test: Call runPreclose twice when blocked, verify only one OPEN alert

**Mitigation**: Use `upsert` with unique key fields as `where` clause.

---

## H5: Approval Gating Breaks Existing Close Flow

**Hypothesis**: Adding approval requirement may break existing `closePeriod` calls that don't expect approval gating, causing regression in M12.1/M12.2 tests.

**Confidence**: HIGH (90%)

**Failure Mode**: Existing tests fail, production close workflow broken.

**Validation**:
- Make approval optional initially: if no approval configured, allow direct close
- OR: Add feature flag / org setting for approval requirement
- Preferred: Approval required by default, but L5 can always force-close
- Update existing E2E tests to either create approved request OR use force-close

**Mitigation**: closePeriod checks: if approval required → verify APPROVED request OR forceClose with reason.

---

## H6: Export Hash Mismatch Due to BOM/Newlines on Windows

**Hypothesis**: If dashboard exports data or displays close pack hashes, the hash may differ between platforms due to BOM or CRLF vs LF line endings.

**Confidence**: LOW (40%)

**Failure Mode**: Hash verification fails on different platforms.

**Validation**:
- Dashboard only displays existing close pack hash from DB, doesn't regenerate
- Hash is computed at close time with consistent encoding (UTF-8 without BOM)
- This is informational display, not verification at dashboard level

**Mitigation**: Dashboard displays stored hash only; close pack generation uses Buffer with explicit encoding.

---

## H7: Tests Hang Due to Open Handles

**Hypothesis**: New services or controllers may leave open handles (database connections, event emitters) causing tests to hang.

**Confidence**: MEDIUM (75%)

**Failure Mode**: E2E tests hang, CI timeout, requires --forceExit.

**Validation**:
- Use existing `cleanup(app)` pattern in afterAll
- Run with --detectOpenHandles flag
- No setInterval/setTimeout without cleanup
- Verify with `test:e2e:teardown-check`

**Mitigation**: Follow E2E_NO_HANG_STANDARD.md; use existing service patterns that cleanup properly.

---

## H8: PRE-015 Fix Accidentally Weakens Assertions

**Hypothesis**: When fixing the `type` → `locationType` field in test files, assertions may be accidentally removed or weakened.

**Confidence**: LOW (30%)

**Failure Mode**: Tests pass but don't actually verify behavior, false confidence.

**Validation**:
- Fix is purely field rename: `type: 'STORE'` → `locationType: 'STORAGE'`
- No changes to assertions or expectations
- Run fixed tests to verify they pass
- Manual review of diff

**Mitigation**: Minimal change - only update the field name in create data, don't touch assertions.

---

## H9: Close Request Unique Constraint Allows Multiple Requests Per Period

**Hypothesis**: If unique constraint on `periodId` is not enforced, multiple close requests could be created for the same period.

**Confidence**: HIGH (90%)

**Failure Mode**: Conflicting approval states, unclear which request is valid.

**Validation**:
- Schema: `periodId String @unique` (one request per period)
- Service: Check for existing request before creating new one
- E2E test: Try to create second request for same period → should fail

**Mitigation**: Add `@unique` constraint on periodId in schema; service returns existing if found.

---

## H10: Dashboard Filters Don't Respect User's Branch Access

**Hypothesis**: If user has access only to specific branches, dashboard may return data for all branches in the org.

**Confidence**: MEDIUM (60%)

**Failure Mode**: Users see data for branches they shouldn't access.

**Validation**:
- Check user's roleAssignments for branch-level access
- Filter dashboard results to only accessible branches
- OR: Return all branches (L4+ org-level access assumed for period management)

**Mitigation**: For M12.4, assume L4+ has org-level visibility for period dashboard. Future: Add branch-level filtering based on roleAssignments.

---

## Summary Table

| ID | Hypothesis | Confidence | Failure Mode |
|----|------------|------------|--------------|
| H1 | Cross-branch leakage | 95% | Data leak |
| H2 | Force-close no audit | 90% | Compliance gap |
| H3 | N+1 query dashboard | 70% | Performance |
| H4 | Alert duplicates | 85% | Alert spam |
| H5 | Approval breaks existing | 90% | Regression |
| H6 | Hash mismatch Windows | 40% | Display issue |
| H7 | Test hang | 75% | CI failure |
| H8 | Weakened assertions | 30% | Test quality |
| H9 | Multiple requests | 90% | State conflict |
| H10 | Branch access filter | 60% | Visibility leak |

---

**Report Generated**: 2026-01-07  
**Agent**: GitHub Copilot (Claude Opus 4.5)
