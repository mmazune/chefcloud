# M16 â€“ Step 2: Offline-First & Sync Design

**Date**: 2025-11-21  
**Status**: DESIGN ONLY (No Code Changes)  
**Purpose**: Define practical offline/sync model for POS, KDS, and booking portal clients.

---

## 1. Design Principles

### 1.1 Server as Source of Truth

- **Server canonical**: In conflict scenarios, server state wins
- **Client state is transient**: Offline changes are a queue waiting to replay
- **No distributed consensus**: Clients don't sync with each other, only with server

### 1.2 Graceful Degradation

- **POS must work offline**: Waiters can take orders during network outages
- **KDS can tolerate short disconnects**: Tickets are cached, updates queued
- **Booking portal is online-only**: Guest bookings require server (payment/deposit flow)

### 1.3 Simple Conflict Resolution

- **Last-write-wins** for updates to same entity (rare case)
- **Idempotency keys prevent duplicates** (primary concern)
- **Server rejects invalid state transitions** (e.g., can't seat a cancelled reservation)

---

## 2. POS Offline Model

### 2.1 Client-Side Architecture

**Local Queue Structure** (localStorage/IndexedDB):
```typescript
interface OfflineQueueEntry {
  id: string;                    // ULID generated by client
  operation: 'CREATE_ORDER' | 'SEND_TO_KITCHEN' | 'CLOSE_ORDER' | 'MODIFY_ORDER';
  payload: any;                  // Request body
  idempotencyKey: string;        // For server-side duplicate prevention
  timestamp: Date;               // When operation was queued
  retryCount: number;            // Number of replay attempts
  status: 'PENDING' | 'REPLAYING' | 'SUCCESS' | 'FAILED';
  errorMessage?: string;         // If replay failed
}
```

### 2.2 Lifecycle: Create Order

**Online**:
1. Waiter adds items to order
2. Client POST `/pos/orders` with `Idempotency-Key: <ULID>`
3. Server creates order, returns order ID
4. Client stores order locally, marks synced

**Offline**:
1. Waiter adds items to order
2. Client generates local order ID (ULID)
3. Client stores order in OfflineQueue with status=PENDING
4. Client displays order in UI (marked as "Not synced")
5. Background sync loop detects network restored â†’ replays queue

**Replay Logic**:
```typescript
async function replayQueue() {
  const pending = await queue.getAll({ status: 'PENDING' });
  
  for (const entry of pending) {
    try {
      await queue.update(entry.id, { status: 'REPLAYING' });
      
      const response = await fetch('/pos/orders', {
        method: 'POST',
        headers: {
          'Idempotency-Key': entry.idempotencyKey,
          'Authorization': `Bearer ${token}`
        },
        body: JSON.stringify(entry.payload)
      });
      
      if (response.ok) {
        await queue.update(entry.id, { status: 'SUCCESS' });
      } else if (response.status === 409) {
        // Server already processed this (idempotency hit)
        await queue.update(entry.id, { status: 'SUCCESS' });
      } else if (response.status === 422) {
        // Validation error - mark failed, alert user
        await queue.update(entry.id, { 
          status: 'FAILED', 
          errorMessage: await response.text() 
        });
      } else {
        // Transient error - retry later
        await queue.update(entry.id, { retryCount: entry.retryCount + 1 });
      }
    } catch (error) {
      // Network still down - keep in queue
      await queue.update(entry.id, { retryCount: entry.retryCount + 1 });
    }
  }
}
```

### 2.3 Lifecycle: Send to Kitchen

**Offline Handling**:
- If order was created offline and not yet synced:
  - Client keeps "SENT" status locally
  - When order syncs, immediately POST `/pos/orders/:id/send-to-kitchen`
- If order exists on server but network down:
  - Queue the "send" operation
  - Replay when network restored

**Constraint**: KDS won't receive ticket until POS syncs. Acceptable - network must be restored for kitchen to operate.

### 2.4 Lifecycle: Close Order (Payment)

**Offline Handling**:
- **CASH payments**: Can be taken offline
  - Client calculates total locally
  - Queue close operation with payment details
  - Replay when online â†’ server posts GL entries
- **Card/MoMo payments**: **REQUIRE ONLINE** (payment gateway)
  - Client blocks payment if offline
  - User must wait for network or switch to cash

### 2.5 Conflict Scenarios

#### Scenario A: Duplicate Order (Same Items)

**Problem**: Waiter clicks "Create Order" twice, once offline (queued), once online (direct).

**Solution**: Idempotency key prevents duplicate. 
- Offline request: `Idempotency-Key: order-abc123`
- Online request: `Idempotency-Key: order-abc123` (same ULID)
- Server returns same order ID both times

#### Scenario B: Modified Order (Offline Then Online)

**Problem**: Order modified offline, then modified again online before sync.

**Solution**: Last-write-wins (server state wins).
1. Offline modification stored in queue
2. Online modification hits server directly
3. Offline modification replays â†’ server sees newer `updatedAt` â†’ rejects with 409 Conflict
4. Client fetches latest order state from server, discards offline change, alerts user

**Alternative (safer)**: Client checks `updatedAt` before replaying. If server's `updatedAt` is newer than queued operation's timestamp, skip replay and alert user.

---

## 3. KDS Offline Model

### 3.1 Client-Side Architecture

**Ticket Cache** (IndexedDB):
```typescript
interface CachedKdsTicket {
  id: string;
  orderId: string;
  station: string;
  status: 'QUEUED' | 'IN_PROGRESS' | 'READY';
  sentAt: Date;
  updatedAt: Date;
  items: KdsTicketItem[];
  waiterName: string;
  slaState: 'GREEN' | 'ORANGE' | 'RED';
}
```

### 3.2 Incremental Sync (Online)

**Polling Loop** (every 5 seconds):
```typescript
let lastSyncAt = new Date('2025-11-21T00:00:00Z');

async function syncKdsTickets(station: string) {
  const response = await fetch(
    `/kds/queue?station=${station}&since=${lastSyncAt.toISOString()}`
  );
  
  if (response.ok) {
    const tickets = await response.json();
    
    // Update local cache
    for (const ticket of tickets) {
      await cache.upsert(ticket.id, ticket);
    }
    
    lastSyncAt = new Date(); // Update sync cursor
    
    // Re-render UI with updated tickets
    renderTickets(await cache.getAll({ station, status: ['QUEUED', 'IN_PROGRESS'] }));
  }
}
```

### 3.3 Offline Handling (Short Disconnects)

**Scenario**: Network drops for 2-3 minutes during busy service.

**Behavior**:
1. KDS continues displaying cached tickets
2. Chef marks tickets as ready (stored locally)
3. SLA timers continue ticking (calculated locally)
4. When network restored:
   - POST `/kds/tickets/:id/mark-ready` for all queued status changes
   - Resume incremental sync to fetch new tickets

**Limitation**: New orders won't appear until network restored (acceptable - POS also offline).

### 3.4 Conflict Resolution

**Problem**: Chef marks ticket ready offline, but server already marked it ready (duplicate POST from another KDS station).

**Solution**: Server idempotency on `mark-ready` endpoint.
- First POST: 200 OK, ticket marked ready
- Second POST (replay): 200 OK, no-op (already ready)

---

## 4. Booking Portal Offline Model

### 4.1 Design Decision: Online-Only

**Rationale**:
- Guest bookings involve payment intents (Flutterwave/MTN MoMo) â†’ requires online payment gateway
- Deposit capture requires real-time GL posting â†’ requires database write
- Availability checks require live data (other guests booking concurrently)

**Behavior**:
- If network down: Display "Service temporarily unavailable" page
- If payment gateway down: Queue reservation in HELD status, retry payment later (handled by worker)

### 4.2 Transient Failure Handling (Idempotency)

**Scenario**: Guest submits booking form, network times out after server receives request but before response reaches browser.

**Client Retry**:
```typescript
async function createBooking(data: BookingFormData) {
  const idempotencyKey = generateULID(); // Store in sessionStorage
  
  const response = await fetch('/public/reservations', {
    method: 'POST',
    headers: {
      'Idempotency-Key': idempotencyKey,
      'Content-Type': 'application/json'
    },
    body: JSON.stringify(data)
  });
  
  if (response.status === 409) {
    // Server already processed this booking
    const booking = await response.json();
    return booking;
  }
  
  return response.json();
}
```

**Server Behavior**: 
- First request: Creates reservation, returns booking ID
- Second request (same idempotency key): Returns existing reservation (no duplicate)

---

## 5. Idempotency Key Conventions

### 5.1 Header-Based (Primary Method)

**Format**: 
```http
Idempotency-Key: <ULID>
```

**Example**:
```http
POST /pos/orders
Idempotency-Key: 01HWABCD1234567890ABCDEFGH
Content-Type: application/json
Authorization: Bearer <token>

{
  "tableId": "table-5",
  "items": [...]
}
```

### 5.2 Body-Based (Fallback for Clients Without Header Support)

**Format**:
```json
{
  "_idempotencyKey": "01HWABCD1234567890ABCDEFGH",
  "tableId": "table-5",
  "items": [...]
}
```

Server extracts `_idempotencyKey` from body, validates, then removes before processing.

### 5.3 Key Generation Rules

**Client-Side** (JavaScript/TypeScript):
```typescript
import { ulid } from 'ulid';

const idempotencyKey = ulid(); // Time-ordered, globally unique
```

**Key Properties**:
- Must be unique per operation attempt
- Should be generated BEFORE API call (stored in client state)
- Can be reused for retries of SAME operation

**Scope**: Per-endpoint, per-operation.
- âœ… Same key for retrying "create order #123"
- âŒ Same key for "create order #123" and "create order #456"

### 5.4 Server Storage

**IdempotencyKey Table**:
```prisma
model IdempotencyKey {
  id            String   @id @default(cuid())
  key           String   @unique // The idempotency key
  endpoint      String   // e.g., "POST /pos/orders"
  requestHash   String   // SHA256(JSON.stringify(requestBody))
  responseBody  Json?    // Cached response to return on duplicate
  statusCode    Int      // HTTP status code of original response
  createdAt     DateTime @default(now())
  expiresAt     DateTime // Auto-expire after 24 hours

  @@index([key, endpoint])
  @@index([expiresAt]) // For cleanup job
}
```

**TTL**: 24 hours (sufficient for client retries, avoids infinite storage growth)

---

## 6. Error Code Conventions

### 6.1 For Clients to React To

| Status | Meaning | Client Action |
|--------|---------|---------------|
| 200 OK | Success | Mark operation complete, remove from queue |
| 201 Created | Resource created | Same as 200 |
| 409 Conflict | Duplicate (idempotency hit) OR entity state conflict | Check response - if idempotency duplicate, mark success; if state conflict, alert user |
| 422 Unprocessable Entity | Validation error (bad data) | Mark failed, alert user to fix data, do not retry |
| 429 Too Many Requests | Rate limit exceeded | Backoff exponentially, retry after delay |
| 500 Internal Server Error | Server bug | Log error, retry with backoff (may be transient DB issue) |
| 503 Service Unavailable | Server overloaded or DB down | Retry with exponential backoff |

### 6.2 Response Format for 409 Conflict

**Idempotency Duplicate**:
```json
{
  "statusCode": 409,
  "message": "Idempotency key already used",
  "conflictType": "IDEMPOTENCY_DUPLICATE",
  "existingResource": {
    "id": "order-abc123",
    "status": "NEW",
    "total": 25000
  }
}
```

**State Conflict**:
```json
{
  "statusCode": 409,
  "message": "Cannot modify order in status CLOSED",
  "conflictType": "INVALID_STATE_TRANSITION",
  "currentState": {
    "id": "order-abc123",
    "status": "CLOSED",
    "updatedAt": "2025-11-21T14:35:00Z"
  }
}
```

---

## 7. Client Responsibilities

### 7.1 POS Client

**Must Implement**:
1. **Offline Queue**: IndexedDB storage for pending operations
2. **Idempotency Key Generation**: ULID per operation
3. **Sync Loop**: Replay queue when network restored
4. **Conflict Handling**: Alert user if server rejects queued operation due to state conflict
5. **UI Indicators**: Show "Not synced" badge on offline orders

**Optional Enhancements**:
6. **Background Sync API**: Use Service Worker for automatic replay when browser closed
7. **Optimistic UI**: Show operation success immediately, rollback if replay fails

### 7.2 KDS Client

**Must Implement**:
1. **Ticket Cache**: IndexedDB for active tickets
2. **Incremental Sync**: Poll `/kds/queue?since=<timestamp>` every 5 seconds
3. **Offline Queue**: Store status changes (mark-ready, bump) for replay
4. **SLA Timer**: Calculate elapsed time locally, don't rely on server

**Nice-to-Have**:
5. **WebSocket Fallback**: For real-time ticket updates (avoids polling)

### 7.3 Booking Portal (Web)

**Must Implement**:
1. **Idempotency Keys**: Include in all POST requests (create booking, confirm)
2. **Retry Logic**: Retry failed requests with same idempotency key
3. **Loading States**: Show spinner during payment intent creation

**Not Needed**:
- Offline queue (online-only by design)
- Local cache (server is canonical)

---

## 8. Server Responsibilities (Step 3)

Will be implemented in **Step 3 - Idempotency & Safe Writes**:

1. **IdempotencyKey Table**: Store keys with 24-hour TTL
2. **Idempotency Middleware**: Check key before processing request
3. **Duplicate Detection**: Return cached response for duplicate keys
4. **Request Fingerprinting**: SHA256 hash of request body to detect modified retries

---

## 9. Sync Status UI Indicators

### 9.1 POS

**Order List**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Order #1234  Table 5      $25,000  â”‚
â”‚ âœ… Synced                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Order #1235  Table 8      $18,000  â”‚
â”‚ ğŸ”„ Syncing... (2 operations pending)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Order #1236  Table 3      $42,000  â”‚
â”‚ âš ï¸ Sync failed - Tap to retry      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 9.2 KDS

**Connection Status Banner**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸŸ¢ Connected - Last sync: 3 seconds ago â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸŸ¡ Reconnecting... (2 updates queued)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ”´ Offline - Showing cached tickets     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 10. Performance Considerations

### 10.1 Queue Size Limits

**Max Queue Size**: 100 operations per client
- If queue exceeds 100, alert user: "Too many offline operations. Please connect to sync."
- Prevents unbounded local storage growth

### 10.2 Replay Batch Size

**Max Concurrent Replays**: 5 operations at once
- Prevents overwhelming server on reconnection
- Ensures operations replay in order (FIFO)

### 10.3 Exponential Backoff

**Retry Delays**:
- Attempt 1: Immediate
- Attempt 2: 5 seconds
- Attempt 3: 15 seconds
- Attempt 4: 45 seconds
- Attempt 5+: 120 seconds (max)

**Max Retries**: 10 attempts, then mark failed and alert user

---

## 11. Testing Scenarios

### 11.1 POS Offline Workflow

1. **Setup**: Disconnect network on POS device
2. **Create Order**: Add items, save order (stored locally)
3. **Send to Kitchen**: Mark order sent (stored locally)
4. **Reconnect Network**: Trigger sync
5. **Verify**: Order appears on server, KDS receives ticket

### 11.2 KDS Offline Workflow

1. **Setup**: KDS client has 10 tickets cached
2. **Disconnect Network**: Unplug ethernet
3. **Mark Tickets Ready**: Chef bumps 3 tickets (stored locally)
4. **Wait 2 Minutes**: Verify cached tickets still display
5. **Reconnect Network**: Trigger sync
6. **Verify**: Server reflects ready status, other stations see updates

### 11.3 Idempotency Test

1. **Create Order**: POST `/pos/orders` with `Idempotency-Key: test-key-1`
2. **Duplicate Request**: POST same endpoint with same key and body
3. **Verify**: Server returns same order ID, no duplicate in database
4. **Modified Request**: POST same key but different items
5. **Verify**: Server returns 409 Conflict with fingerprint mismatch

---

## 12. Known Limitations

### 12.1 No Client-to-Client Sync

- POS devices don't sync with each other
- If waiter modifies order on Device A, Device B won't see change until sync with server
- **Mitigation**: Polling server for order updates (Step 6 observability)

### 12.2 No Distributed Transactions

- If order created offline, payment is cash-only until synced
- Cannot apply prepaid credits or run promotions offline (require server data)

### 12.3 No Real-Time Collaboration

- Multiple staff can't edit same order simultaneously while offline
- Last-write-wins when both sync

---

## 13. Future Enhancements (Post-M16)

### 13.1 WebSocket for Real-Time Updates

- KDS tickets pushed to clients instantly (no polling)
- POS receives order updates from other devices in real-time

### 13.2 Operational Transform (OT) for Conflict Resolution

- Instead of last-write-wins, intelligently merge concurrent edits
- Example: Waiter A adds item X, Waiter B adds item Y â†’ both items appear

### 13.3 Full Client-Side Database (PouchDB + CouchDB Sync)

- Replicate entire menu, orders, inventory to local database
- Bi-directional sync with conflict resolution
- **Complexity**: High, defer until proven need

---

## Conclusion

This design balances **pragmatism** (simple last-write-wins) with **safety** (idempotency keys prevent duplicates). 

**Key Takeaway**: 
- POS offline queue + idempotency = Safe order creation during network blips
- KDS incremental sync + cache = Continuous operation during short disconnects
- Booking portal = Online-only (acceptable for payment flow)

**Next Step**: Implement idempotency infrastructure in **Step 3**.
